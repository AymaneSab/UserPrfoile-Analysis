# Real-Time Data Pipeline Project

In a world where data is considered the new gold, organizations must be able to process and analyze data in real-time to make informed decisions. This project is designed for data professionals seeking practical skills in implementing real-time data pipelines.

## Project Overview

The goal of this project is to develop a real-time data pipeline for processing and analyzing user data generated by randomuser.me. The pipeline includes the following steps:

### Configuration and Dependencies

- Ensure that PySpark is installed and configured.
- Install the necessary libraries and dependencies for Kafka, Cassandra, and MongoDB (use Docker images if possible).
- Download the required JAR files and place them in the specified directory (e.g., D://spark_dependency_jars).

### Define Functions to Save Data

- `save_to_cassandra_table`: This function will be customized to save transformed user profile data to a Cassandra table.
- `save_to_mongodb_collection`: This function will be customized to save aggregated or analyzed data to a MongoDB collection.

### Initialize Spark Session

- Create a Spark session with the necessary configurations, including paths to dependency JARs.

### Read from Kafka

- Stream data from a Kafka topic (e.g., user_profiles).

### Data Transformation

- Define the schema for incoming data based on the structure of randomuser.me.
- Parse incoming Kafka messages and extract relevant fields.
- Perform transformations (e.g., construct full names, validate or recalculate ages, build complete addresses).

### Save Transformed Data to Cassandra

- Save the transformed user profile data to a Cassandra table using the `save_to_cassandra_table` function.

### Data Aggregation

- Aggregate data to extract insights, such as the number of users by nationality, average user age, and most common email domains.
- Save aggregated results to MongoDB collections using the `save_to_mongodb_collection` function.

### Debugging and Monitoring

- Monitor the console output for aggregated results and any potential errors using the `logging` library and `try except` blocks.
- Verify that data is correctly written to the Cassandra table and MongoDB collections.

### Data Visualization

- Create dashboards using Python Dash to visualize aggregated data stored in MongoDB.

### GDPR Documentation

- Document a registry detailing all personal data processing activities, including the types of data stored, the processing purposes, and security measures in place.

### Personal Data Sorting Procedures

- Data Identification: Use data analysis tools to identify personal data stored in the data lake.
- Sorting and Deletion Procedures: Write and implement procedures for sorting and deleting unnecessary or outdated personal data in compliance with GDPR.

### Access Updates

- Adjust access rights in data ingestion tools in Apache Kafka to reflect organizational changes.
- Use the `kafka-acls.sh` script to add ACLs, for example, to grant a user read access to a topic:

```bash
kafka-acls.sh --add --allow-principal User:username --operation Read --topic topicname --group groupname
